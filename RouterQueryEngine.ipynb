{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Y3gSVdHdzb"
      },
      "source": [
        "##### Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOZ6iXTJHlQ5",
        "outputId": "99bf7bd6-1628-4b78-f6cc-4c1ddd572483"
      },
      "outputs": [],
      "source": [
        "!pip install llama_index==0.9.31"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R_Ttz5WDI8M6"
      },
      "outputs": [],
      "source": [
        "# NOTE: This is ONLY necessary in jupyter notebook.\n",
        "# Details: Jupyter runs an event-loop behind the scenes.\n",
        "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
        "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGEOh3sdFuQ-"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\".env\", override=True)\n",
        "openai_key = os.environ[\"OPENAI_API_KEY\"] \n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "logging.getLogger().handlers = []\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
        "\n",
        "from llama_index import (\n",
        "    VectorStoreIndex,\n",
        "    ListIndex,\n",
        "    SimpleKeywordTableIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        "    StorageContext,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TozkIS3mF8DC",
        "outputId": "692030d0-b3cd-417d-856e-21f95bb3d455"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# initialize service context (set chunk size)\n",
        "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
        "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# initialize storage context (by default it's in-memory)\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4LNrmhrK4xe"
      },
      "source": [
        "##### Define List Index and Vector Index over Same Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B9C2Gm1UF8r3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        }
      ],
      "source": [
        "list_index = ListIndex(nodes, storage_context=storage_context)\n",
        "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voktqsQNLApj"
      },
      "source": [
        "##### Define Query Engines and Set Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uLMKbMAUGA9f"
      },
      "outputs": [],
      "source": [
        "list_query_engine = list_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fg4aOwPaGNxb"
      },
      "outputs": [],
      "source": [
        "from llama_index.tools.query_engine import QueryEngineTool\n",
        "\n",
        "\n",
        "list_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=list_query_engine,\n",
        "    description=\"Useful for summarization questions related to Paul Graham eassy on What I Worked On.\",\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context from Paul Graham essay on What I Worked On.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0zHAQS_LF3r"
      },
      "source": [
        "##### Define Router Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GGjl2y5QGRcW"
      },
      "outputs": [],
      "source": [
        "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
        "\n",
        "from llama_index.selectors.pydantic_selectors import (\n",
        "    PydanticSingleSelector\n",
        ")\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        list_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0hywia-DGTx2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Selecting query engine 0: This choice is specifically mentioned as useful for summarization questions..\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "=========\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"The document provides a detailed account of an individual's journey through various fields, including writing, programming, art, and technology. It follows the narrative of the author's exploration of different interests, experiences in art school, work in the tech industry, and entrepreneurial endeavors. The story highlights the individual's transitions between painting, programming, and investing, showcasing a diverse and impactful career trajectory. It also delves into the development of the Lisp programming language, the creation of new Lisp dialects, and the challenges faced during these projects. Personal reflections on experiences, decisions, and career shifts are woven throughout the narrative, offering insights into the individual's evolving interests and ventures.\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = query_engine.query(\"What is the summary of the document?\")\n",
        "print(\"=========\")\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Selecting query engine 1: The question is asking for specific context from Paul Graham's essay on What I Worked On..\n",
            "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "=========\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'After leaving Y Combinator, Paul Graham decided to focus on painting.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = query_engine.query(\"What did Paul Graham do after RICS?\")\n",
        "print(\"=========\")\n",
        "response.response"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
